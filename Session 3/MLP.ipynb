{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941548cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step: 1, loss: 0.6641766428947449\n",
      "step: 2, loss: 0.0003102297196164727\n",
      "step: 3, loss: 1.535407932351518e-06\n",
      "step: 4, loss: 5.483625642455081e-08\n",
      "step: 5, loss: 4.76837147544984e-09\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 21.626018524169922\n",
      "step: 8, loss: 30.5371150970459\n",
      "step: 9, loss: 25.27849578857422\n",
      "step: 10, loss: 18.193992614746094\n",
      "step: 11, loss: 11.885140419006348\n",
      "step: 12, loss: 4.511019706726074\n",
      "step: 13, loss: 1.0964460372924805\n",
      "step: 14, loss: 0.0067946952767670155\n",
      "step: 15, loss: 18.916074752807617\n",
      "step: 16, loss: 21.980440139770508\n",
      "step: 17, loss: 20.16739273071289\n",
      "step: 18, loss: 17.66501808166504\n",
      "step: 19, loss: 14.715853691101074\n",
      "step: 20, loss: 11.280372619628906\n",
      "step: 21, loss: 8.379359245300293\n",
      "step: 22, loss: 4.057778358459473\n",
      "step: 23, loss: 18.58267593383789\n",
      "step: 24, loss: 17.833145141601562\n",
      "step: 25, loss: 17.59850311279297\n",
      "step: 26, loss: 15.649388313293457\n",
      "step: 27, loss: 13.136399269104004\n",
      "step: 28, loss: 10.086899757385254\n",
      "step: 29, loss: 7.391134262084961\n",
      "step: 30, loss: 4.841897487640381\n",
      "step: 31, loss: 11.5562162399292\n",
      "step: 32, loss: 10.439371109008789\n",
      "step: 33, loss: 9.411323547363281\n",
      "step: 34, loss: 7.981155872344971\n",
      "step: 35, loss: 6.412858963012695\n",
      "step: 36, loss: 4.8150715827941895\n",
      "step: 37, loss: 2.564232110977173\n",
      "step: 38, loss: 5.786968231201172\n",
      "step: 39, loss: 11.087267875671387\n",
      "step: 40, loss: 9.993622779846191\n",
      "step: 41, loss: 8.619991302490234\n",
      "step: 42, loss: 7.3706560134887695\n",
      "step: 43, loss: 5.563217639923096\n",
      "step: 44, loss: 3.738640069961548\n",
      "step: 45, loss: 2.102790594100952\n",
      "step: 46, loss: 9.871003150939941\n",
      "step: 47, loss: 18.33201026916504\n",
      "step: 48, loss: 17.215526580810547\n",
      "step: 49, loss: 15.775615692138672\n",
      "step: 50, loss: 14.383925437927246\n",
      "step: 51, loss: 12.73416519165039\n",
      "step: 52, loss: 11.28842544555664\n",
      "step: 53, loss: 9.272331237792969\n",
      "step: 54, loss: 7.652369499206543\n",
      "step: 55, loss: 6.453456401824951\n",
      "step: 56, loss: 5.408489227294922\n",
      "step: 57, loss: 3.9082610607147217\n",
      "step: 58, loss: 3.038498878479004\n",
      "step: 59, loss: 1.988204836845398\n",
      "step: 60, loss: 1.0810405015945435\n",
      "step: 61, loss: 0.7625190615653992\n",
      "step: 62, loss: 8.543923377990723\n",
      "step: 63, loss: 9.269048690795898\n",
      "step: 64, loss: 8.495967864990234\n",
      "step: 65, loss: 7.978490829467773\n",
      "step: 66, loss: 7.445216178894043\n",
      "step: 67, loss: 6.945990562438965\n",
      "step: 68, loss: 6.374744892120361\n",
      "step: 69, loss: 5.773571968078613\n",
      "step: 70, loss: 8.490694046020508\n",
      "step: 71, loss: 8.890654563903809\n",
      "step: 72, loss: 8.646015167236328\n",
      "step: 73, loss: 7.9313740730285645\n",
      "step: 74, loss: 7.633089065551758\n",
      "step: 75, loss: 7.317915439605713\n",
      "step: 76, loss: 6.911293983459473\n",
      "step: 77, loss: 6.44705057144165\n",
      "step: 78, loss: 8.655521392822266\n",
      "step: 79, loss: 8.256946563720703\n",
      "step: 80, loss: 8.010684967041016\n",
      "step: 81, loss: 7.571961879730225\n",
      "step: 82, loss: 7.478393077850342\n",
      "step: 83, loss: 6.867598056793213\n",
      "step: 84, loss: 6.657144069671631\n",
      "step: 85, loss: 6.390041351318359\n",
      "step: 86, loss: 10.901557922363281\n",
      "step: 87, loss: 10.328516006469727\n",
      "step: 88, loss: 10.12662124633789\n",
      "step: 89, loss: 10.026329040527344\n",
      "step: 90, loss: 9.603616714477539\n",
      "step: 91, loss: 9.401962280273438\n",
      "step: 92, loss: 9.11098861694336\n",
      "step: 93, loss: 8.841863632202148\n",
      "step: 94, loss: 6.365707874298096\n",
      "step: 95, loss: 6.32282829284668\n",
      "step: 96, loss: 5.918012619018555\n",
      "step: 97, loss: 5.6770830154418945\n",
      "step: 98, loss: 5.472324848175049\n",
      "step: 99, loss: 5.278947353363037\n",
      "step: 100, loss: 5.001543045043945\n",
      "step: 101, loss: 5.3117241859436035\n",
      "step: 102, loss: 8.642372131347656\n",
      "step: 103, loss: 8.536487579345703\n",
      "step: 104, loss: 8.528851509094238\n",
      "step: 105, loss: 8.313316345214844\n",
      "step: 106, loss: 7.925917148590088\n",
      "step: 107, loss: 7.751783847808838\n",
      "step: 108, loss: 7.3798322677612305\n",
      "step: 109, loss: 7.529473781585693\n",
      "step: 110, loss: 8.711613655090332\n",
      "step: 111, loss: 8.531137466430664\n",
      "step: 112, loss: 8.274709701538086\n",
      "step: 113, loss: 8.0755615234375\n",
      "step: 114, loss: 7.8261260986328125\n",
      "step: 115, loss: 7.56365966796875\n",
      "step: 116, loss: 7.336162090301514\n",
      "step: 117, loss: 7.279867649078369\n",
      "step: 118, loss: 7.552700996398926\n",
      "step: 119, loss: 7.3132548332214355\n",
      "step: 120, loss: 7.049767971038818\n",
      "step: 121, loss: 6.7843194007873535\n",
      "step: 122, loss: 6.547258377075195\n",
      "step: 123, loss: 6.255317211151123\n",
      "step: 124, loss: 6.034304618835449\n",
      "step: 125, loss: 6.897977352142334\n",
      "step: 126, loss: 8.747511863708496\n",
      "step: 127, loss: 8.509093284606934\n",
      "step: 128, loss: 8.342737197875977\n",
      "step: 129, loss: 8.283287048339844\n",
      "step: 130, loss: 7.833343029022217\n",
      "step: 131, loss: 7.644744396209717\n",
      "step: 132, loss: 7.316352367401123\n",
      "step: 133, loss: 6.6552042961120605\n",
      "step: 134, loss: 6.501469612121582\n",
      "step: 135, loss: 6.282309532165527\n",
      "step: 136, loss: 6.070770740509033\n",
      "step: 137, loss: 5.835383892059326\n",
      "step: 138, loss: 5.562900543212891\n",
      "step: 139, loss: 5.278529644012451\n",
      "step: 140, loss: 7.201147079467773\n",
      "step: 141, loss: 8.763445854187012\n",
      "step: 142, loss: 8.500527381896973\n",
      "step: 143, loss: 8.344841003417969\n",
      "step: 144, loss: 8.079477310180664\n",
      "step: 145, loss: 7.7820611000061035\n",
      "step: 146, loss: 7.235947132110596\n",
      "step: 147, loss: 6.606555938720703\n",
      "step: 148, loss: 6.335783004760742\n",
      "step: 149, loss: 5.943194389343262\n",
      "step: 150, loss: 2.8484151363372803\n",
      "step: 151, loss: 2.8535962104797363\n",
      "step: 152, loss: 2.793302059173584\n",
      "step: 153, loss: 2.6879351139068604\n",
      "step: 154, loss: 2.866304397583008\n",
      "step: 155, loss: 2.892665386199951\n",
      "step: 156, loss: 2.5472586154937744\n",
      "step: 157, loss: 2.5647776126861572\n",
      "step: 158, loss: 2.326129913330078\n",
      "step: 159, loss: 2.5545108318328857\n",
      "step: 160, loss: 2.425684928894043\n",
      "step: 161, loss: 2.2413599491119385\n",
      "step: 162, loss: 2.4109344482421875\n",
      "step: 163, loss: 2.4049761295318604\n",
      "step: 164, loss: 2.2834317684173584\n",
      "step: 165, loss: 2.3406882286071777\n",
      "step: 166, loss: 2.5717995166778564\n",
      "step: 167, loss: 2.2659783363342285\n",
      "step: 168, loss: 1.9823139905929565\n",
      "step: 169, loss: 2.3980281352996826\n",
      "step: 170, loss: 2.276569128036499\n",
      "step: 171, loss: 2.3189377784729004\n",
      "step: 172, loss: 2.420590877532959\n",
      "step: 173, loss: 2.1472890377044678\n",
      "step: 174, loss: 1.8224765062332153\n",
      "step: 175, loss: 2.0614871978759766\n",
      "step: 176, loss: 1.799087405204773\n",
      "step: 177, loss: 1.9121344089508057\n",
      "step: 178, loss: 1.972286581993103\n",
      "step: 179, loss: 1.994866967201233\n",
      "step: 180, loss: 1.9143953323364258\n",
      "step: 181, loss: 2.026094675064087\n",
      "step: 182, loss: 1.9187219142913818\n",
      "step: 183, loss: 1.7317701578140259\n",
      "step: 184, loss: 1.7716809511184692\n",
      "step: 185, loss: 1.605282187461853\n",
      "step: 186, loss: 1.5488721132278442\n",
      "step: 187, loss: 1.5537524223327637\n",
      "step: 188, loss: 1.515784502029419\n",
      "step: 189, loss: 1.4985994100570679\n",
      "step: 190, loss: 1.4353033304214478\n",
      "step: 191, loss: 1.6870907545089722\n",
      "step: 192, loss: 1.5177677869796753\n",
      "step: 193, loss: 1.6638826131820679\n",
      "step: 194, loss: 1.5715793371200562\n",
      "step: 195, loss: 1.27751886844635\n",
      "step: 196, loss: 1.7928770780563354\n",
      "step: 197, loss: 1.4392790794372559\n",
      "step: 198, loss: 1.5715622901916504\n",
      "step: 199, loss: 1.1808955669403076\n",
      "step: 200, loss: 1.326686978340149\n",
      "step: 201, loss: 1.4299230575561523\n",
      "step: 202, loss: 1.4177196025848389\n",
      "step: 203, loss: 1.0685207843780518\n",
      "step: 204, loss: 1.1397099494934082\n",
      "step: 205, loss: 1.0691139698028564\n",
      "step: 206, loss: 1.3897128105163574\n",
      "step: 207, loss: 1.1918728351593018\n",
      "step: 208, loss: 1.2067183256149292\n",
      "step: 209, loss: 1.2327779531478882\n",
      "step: 210, loss: 1.151736855506897\n",
      "step: 211, loss: 0.7491396069526672\n",
      "step: 212, loss: 1.078529715538025\n",
      "step: 213, loss: 1.060343861579895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 214, loss: 1.1918617486953735\n",
      "step: 215, loss: 0.9100221395492554\n",
      "step: 216, loss: 0.9691815972328186\n",
      "step: 217, loss: 0.7164879441261292\n",
      "step: 218, loss: 0.9658658504486084\n",
      "step: 219, loss: 0.7401107549667358\n",
      "step: 220, loss: 0.794175922870636\n",
      "step: 221, loss: 0.8878083229064941\n",
      "step: 222, loss: 0.8653994202613831\n",
      "step: 223, loss: 0.7796916365623474\n",
      "step: 224, loss: 1.0862151384353638\n",
      "step: 225, loss: 0.8265483379364014\n",
      "step: 226, loss: 0.9711195230484009\n",
      "step: 227, loss: 1.036349892616272\n",
      "step: 228, loss: 0.7628336548805237\n",
      "step: 229, loss: 0.8410715460777283\n",
      "step: 230, loss: 0.5935696363449097\n",
      "step: 231, loss: 0.8407084941864014\n",
      "step: 232, loss: 0.8712412118911743\n",
      "step: 233, loss: 1.0594398975372314\n",
      "step: 234, loss: 0.6300300359725952\n",
      "step: 235, loss: 0.8060867190361023\n",
      "step: 236, loss: 0.9176609516143799\n",
      "step: 237, loss: 0.847246527671814\n",
      "step: 238, loss: 0.9677011370658875\n",
      "step: 239, loss: 1.169653296470642\n",
      "step: 240, loss: 0.6495435237884521\n",
      "step: 241, loss: 0.7127788662910461\n",
      "step: 242, loss: 0.7773093581199646\n",
      "step: 243, loss: 0.8577945828437805\n",
      "step: 244, loss: 0.7601677775382996\n",
      "step: 245, loss: 0.5085891485214233\n",
      "step: 246, loss: 0.8454031944274902\n",
      "step: 247, loss: 0.41681477427482605\n",
      "step: 248, loss: 0.7555028796195984\n",
      "step: 249, loss: 0.38766711950302124\n",
      "step: 250, loss: 0.3492765724658966\n",
      "step: 251, loss: 0.5632265210151672\n",
      "step: 252, loss: 0.9290623664855957\n",
      "step: 253, loss: 0.6797788739204407\n",
      "step: 254, loss: 0.476820170879364\n",
      "step: 255, loss: 0.21957825124263763\n",
      "step: 256, loss: 0.6338474750518799\n",
      "step: 257, loss: 0.8937933444976807\n",
      "step: 258, loss: 0.8080390691757202\n",
      "step: 259, loss: 0.7473053932189941\n",
      "step: 260, loss: 0.5591424703598022\n",
      "step: 261, loss: 0.6971774101257324\n",
      "step: 262, loss: 0.4899022579193115\n",
      "step: 263, loss: 0.390746146440506\n",
      "step: 264, loss: 0.3677079379558563\n",
      "step: 265, loss: 0.5180619955062866\n",
      "step: 266, loss: 0.5996184945106506\n",
      "step: 267, loss: 0.36185431480407715\n",
      "step: 268, loss: 0.6579739451408386\n",
      "step: 269, loss: 0.6350489258766174\n",
      "step: 270, loss: 0.17781373858451843\n",
      "step: 271, loss: 0.39376747608184814\n",
      "step: 272, loss: 0.6260563135147095\n",
      "step: 273, loss: 0.4303983747959137\n",
      "step: 274, loss: 0.9167227149009705\n",
      "step: 275, loss: 0.19979025423526764\n",
      "step: 276, loss: 0.6315988302230835\n",
      "step: 277, loss: 0.5018303394317627\n",
      "step: 278, loss: 0.5273007154464722\n",
      "step: 279, loss: 0.7215829491615295\n",
      "step: 280, loss: 0.46233341097831726\n",
      "step: 281, loss: 0.31988126039505005\n",
      "step: 282, loss: 0.9416425228118896\n",
      "step: 283, loss: 0.4674120843410492\n",
      "step: 284, loss: 0.3401710093021393\n",
      "step: 285, loss: 0.8352994322776794\n",
      "step: 286, loss: 0.4702613353729248\n",
      "step: 287, loss: 0.5204697847366333\n",
      "step: 288, loss: 0.378330796957016\n",
      "step: 289, loss: 0.9406430125236511\n",
      "step: 290, loss: 0.6906830072402954\n",
      "step: 291, loss: 0.36792251467704773\n",
      "step: 292, loss: 0.26560887694358826\n",
      "step: 293, loss: 0.6666799783706665\n",
      "step: 294, loss: 0.31923145055770874\n",
      "step: 295, loss: 0.31893038749694824\n",
      "step: 296, loss: 0.5449361205101013\n",
      "step: 297, loss: 0.48943495750427246\n",
      "step: 298, loss: 0.6539105176925659\n",
      "step: 299, loss: 0.3751690983772278\n",
      "step: 300, loss: 0.23186509311199188\n",
      "step: 301, loss: 0.24659042060375214\n",
      "step: 302, loss: 0.05470305308699608\n",
      "step: 303, loss: 0.06582441926002502\n",
      "step: 304, loss: 0.19323596358299255\n",
      "step: 305, loss: 0.17019085586071014\n",
      "step: 306, loss: 0.1594081073999405\n",
      "step: 307, loss: 0.2797754406929016\n",
      "step: 308, loss: 0.23167432844638824\n",
      "step: 309, loss: 0.2802584171295166\n",
      "step: 310, loss: 0.1362464874982834\n",
      "step: 311, loss: 0.20357218384742737\n",
      "step: 312, loss: 0.23258769512176514\n",
      "step: 313, loss: 0.23933081328868866\n",
      "step: 314, loss: 0.16577096283435822\n",
      "step: 315, loss: 0.35892415046691895\n",
      "step: 316, loss: 0.4721325933933258\n",
      "step: 317, loss: 0.2569487392902374\n",
      "step: 318, loss: 0.21005432307720184\n",
      "step: 319, loss: 0.2500140070915222\n",
      "step: 320, loss: 0.14033807814121246\n",
      "step: 321, loss: 0.23298366367816925\n",
      "step: 322, loss: 0.22212713956832886\n",
      "step: 323, loss: 0.20438407361507416\n",
      "step: 324, loss: 0.11055809259414673\n",
      "step: 325, loss: 0.09637653082609177\n",
      "step: 326, loss: 0.09208150207996368\n",
      "step: 327, loss: 0.1739863008260727\n",
      "step: 328, loss: 0.396322637796402\n",
      "step: 329, loss: 0.4177069365978241\n",
      "step: 330, loss: 0.10756458342075348\n",
      "step: 331, loss: 0.1083875298500061\n",
      "step: 332, loss: 0.06432323157787323\n",
      "step: 333, loss: 0.19761522114276886\n",
      "step: 334, loss: 0.06816381216049194\n",
      "step: 335, loss: 0.037538353353738785\n",
      "step: 336, loss: 0.14295576512813568\n",
      "step: 337, loss: 0.16621489822864532\n",
      "step: 338, loss: 0.07808883488178253\n",
      "step: 339, loss: 0.07302573323249817\n",
      "step: 340, loss: 0.13432921469211578\n",
      "step: 341, loss: 0.36521780490875244\n",
      "step: 342, loss: 0.12557779252529144\n",
      "step: 343, loss: 0.27795150876045227\n",
      "step: 344, loss: 0.20975732803344727\n",
      "step: 345, loss: 0.10494562238454819\n",
      "step: 346, loss: 0.3880521357059479\n",
      "step: 347, loss: 0.05083797499537468\n",
      "step: 348, loss: 0.1317470371723175\n",
      "step: 349, loss: 0.05567661300301552\n",
      "step: 350, loss: 0.15021932125091553\n",
      "step: 351, loss: 0.03731999546289444\n",
      "step: 352, loss: 0.4806520938873291\n",
      "step: 353, loss: 0.6124931573867798\n",
      "step: 354, loss: 0.19151920080184937\n",
      "step: 355, loss: 0.13686628639698029\n",
      "step: 356, loss: 0.1095796599984169\n",
      "step: 357, loss: 0.057867489755153656\n",
      "step: 358, loss: 0.13938184082508087\n",
      "step: 359, loss: 0.050900351256132126\n",
      "step: 360, loss: 0.27089327573776245\n",
      "step: 361, loss: 0.04374132305383682\n",
      "step: 362, loss: 0.293501615524292\n",
      "step: 363, loss: 0.19539280235767365\n",
      "step: 364, loss: 0.17672376334667206\n",
      "step: 365, loss: 0.12309911847114563\n",
      "step: 366, loss: 0.19580166041851044\n",
      "step: 367, loss: 0.10119105130434036\n",
      "step: 368, loss: 0.2409268170595169\n",
      "step: 369, loss: 0.1531340479850769\n",
      "step: 370, loss: 0.11006515473127365\n",
      "step: 371, loss: 0.2373846024274826\n",
      "step: 372, loss: 0.19153787195682526\n",
      "step: 373, loss: 0.1888015866279602\n",
      "step: 374, loss: 0.13475292921066284\n",
      "step: 375, loss: 0.108607217669487\n",
      "step: 376, loss: 0.18186001479625702\n",
      "step: 377, loss: 0.13483402132987976\n",
      "step: 378, loss: 0.07854615896940231\n",
      "step: 379, loss: 0.1335386037826538\n",
      "step: 380, loss: 0.16685333847999573\n",
      "step: 381, loss: 0.15137843787670135\n",
      "step: 382, loss: 0.2753949463367462\n",
      "step: 383, loss: 0.17626190185546875\n",
      "step: 384, loss: 0.1611587405204773\n",
      "step: 385, loss: 0.16572174429893494\n",
      "step: 386, loss: 0.26313096284866333\n",
      "step: 387, loss: 0.09562515467405319\n",
      "step: 388, loss: 0.05010911449790001\n",
      "step: 389, loss: 0.06449256092309952\n",
      "step: 390, loss: 0.07977674901485443\n",
      "step: 391, loss: 0.3191405236721039\n",
      "step: 392, loss: 0.2494269609451294\n",
      "step: 393, loss: 0.35136327147483826\n",
      "step: 394, loss: 0.08225765824317932\n",
      "step: 395, loss: 0.1261083334684372\n",
      "step: 396, loss: 0.10365819185972214\n",
      "step: 397, loss: 0.10861559212207794\n",
      "step: 398, loss: 0.24345119297504425\n",
      "step: 399, loss: 0.110889732837677\n",
      "step: 400, loss: 0.1739998459815979\n",
      "step: 401, loss: 0.1892675906419754\n",
      "step: 402, loss: 0.13981680572032928\n",
      "step: 403, loss: 0.23265326023101807\n",
      "step: 404, loss: 0.2561612129211426\n",
      "step: 405, loss: 0.13392426073551178\n",
      "step: 406, loss: 0.2761019766330719\n",
      "step: 407, loss: 0.14336101710796356\n",
      "step: 408, loss: 0.038256898522377014\n",
      "step: 409, loss: 0.20029720664024353\n",
      "step: 410, loss: 0.20275287330150604\n",
      "step: 411, loss: 0.1621832698583603\n",
      "step: 412, loss: 0.031069627031683922\n",
      "step: 413, loss: 0.1365884691476822\n",
      "step: 414, loss: 0.1740739941596985\n",
      "step: 415, loss: 0.17522834241390228\n",
      "step: 416, loss: 0.1645769625902176\n",
      "step: 417, loss: 0.2661650776863098\n",
      "step: 418, loss: 0.12002000212669373\n",
      "step: 419, loss: 0.33673015236854553\n",
      "step: 420, loss: 0.167286217212677\n",
      "step: 421, loss: 0.1338188201189041\n",
      "step: 422, loss: 0.1170271709561348\n",
      "step: 423, loss: 0.03697635605931282\n",
      "step: 424, loss: 0.13918690383434296\n",
      "step: 425, loss: 0.0799253061413765\n",
      "step: 426, loss: 0.3965979516506195\n",
      "step: 427, loss: 0.1679334044456482\n",
      "step: 428, loss: 0.318099707365036\n",
      "step: 429, loss: 0.128549724817276\n",
      "step: 430, loss: 0.22627516090869904\n",
      "step: 431, loss: 0.11998894810676575\n",
      "step: 432, loss: 0.1813807636499405\n",
      "step: 433, loss: 0.44963592290878296\n",
      "step: 434, loss: 0.24052824079990387\n",
      "step: 435, loss: 0.0312420055270195\n",
      "step: 436, loss: 0.06942680478096008\n",
      "step: 437, loss: 0.07084004580974579\n",
      "step: 438, loss: 0.2480277121067047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 439, loss: 0.09179869294166565\n",
      "step: 440, loss: 0.3456719219684601\n",
      "step: 441, loss: 0.06741172075271606\n",
      "step: 442, loss: 0.09315009415149689\n",
      "step: 443, loss: 0.20690101385116577\n",
      "step: 444, loss: 0.1642235517501831\n",
      "step: 445, loss: 0.2741878032684326\n",
      "step: 446, loss: 0.18285278975963593\n",
      "step: 447, loss: 0.10836867988109589\n",
      "step: 448, loss: 0.24960510432720184\n",
      "step: 449, loss: 0.14535969495773315\n",
      "step: 450, loss: 0.05338016524910927\n",
      "step: 451, loss: 0.023045353591442108\n",
      "step: 452, loss: 0.0738467276096344\n",
      "step: 453, loss: 0.05253177136182785\n",
      "step: 454, loss: 0.009891383349895477\n",
      "step: 455, loss: 0.009929139167070389\n",
      "step: 456, loss: 0.012635987251996994\n",
      "step: 457, loss: 0.024707332253456116\n",
      "step: 458, loss: 0.01463110651820898\n",
      "step: 459, loss: 0.019016383215785027\n",
      "step: 460, loss: 0.021475594490766525\n",
      "step: 461, loss: 0.009618332609534264\n",
      "step: 462, loss: 0.08267613500356674\n",
      "step: 463, loss: 0.13839298486709595\n",
      "step: 464, loss: 0.04581601917743683\n",
      "step: 465, loss: 0.00807315856218338\n",
      "step: 466, loss: 0.02062033675611019\n",
      "step: 467, loss: 0.007588397711515427\n",
      "step: 468, loss: 0.05275547876954079\n",
      "step: 469, loss: 0.007693027146160603\n",
      "step: 470, loss: 0.004306305665522814\n",
      "step: 471, loss: 0.036547768861055374\n",
      "step: 472, loss: 0.023656608536839485\n",
      "step: 473, loss: 0.019811339676380157\n",
      "step: 474, loss: 0.015134019777178764\n",
      "step: 475, loss: 0.0073253740556538105\n",
      "step: 476, loss: 0.027575679123401642\n",
      "step: 477, loss: 0.0112629234790802\n",
      "step: 478, loss: 0.11574719101190567\n",
      "step: 479, loss: 0.007987496443092823\n",
      "step: 480, loss: 0.013733821921050549\n",
      "step: 481, loss: 0.0633489117026329\n",
      "step: 482, loss: 0.029599254950881004\n",
      "step: 483, loss: 0.019283611327409744\n",
      "step: 484, loss: 0.03472903370857239\n",
      "step: 485, loss: 0.016100779175758362\n",
      "step: 486, loss: 0.01496780477464199\n",
      "step: 487, loss: 0.009395353496074677\n",
      "step: 488, loss: 0.01119357068091631\n",
      "step: 489, loss: 0.01908097416162491\n",
      "step: 490, loss: 0.035510484129190445\n",
      "step: 491, loss: 0.026555614545941353\n",
      "step: 492, loss: 0.014800403267145157\n",
      "step: 493, loss: 0.05941900610923767\n",
      "step: 494, loss: 0.04336449131369591\n",
      "step: 495, loss: 0.05368285998702049\n",
      "step: 496, loss: 0.007388672791421413\n",
      "step: 497, loss: 0.1330603063106537\n",
      "step: 498, loss: 0.01070649828761816\n",
      "step: 499, loss: 0.0227919053286314\n",
      "step: 500, loss: 0.007272052578628063\n",
      "step: 501, loss: 0.02040261961519718\n",
      "step: 502, loss: 0.0044527058489620686\n",
      "step: 503, loss: 0.0049035819247365\n",
      "step: 504, loss: 0.006462248042225838\n",
      "step: 505, loss: 0.026220260187983513\n",
      "step: 506, loss: 0.018259651958942413\n",
      "step: 507, loss: 0.024714395403862\n",
      "step: 508, loss: 0.018649933859705925\n",
      "step: 509, loss: 0.0771254226565361\n",
      "step: 510, loss: 0.01207456924021244\n",
      "step: 511, loss: 0.05635681748390198\n",
      "step: 512, loss: 0.008892628364264965\n",
      "step: 513, loss: 0.03792259842157364\n",
      "step: 514, loss: 0.028847506269812584\n",
      "step: 515, loss: 0.007933574728667736\n",
      "step: 516, loss: 0.08897331357002258\n",
      "step: 517, loss: 0.028973445296287537\n",
      "step: 518, loss: 0.005556458607316017\n",
      "step: 519, loss: 0.027911877259612083\n",
      "step: 520, loss: 0.00616642227396369\n",
      "step: 521, loss: 0.018238279968500137\n",
      "step: 522, loss: 0.0153501583263278\n",
      "step: 523, loss: 0.07288111001253128\n",
      "step: 524, loss: 0.01568339765071869\n",
      "step: 525, loss: 0.04390622675418854\n",
      "step: 526, loss: 0.07144038379192352\n",
      "step: 527, loss: 0.02803162857890129\n",
      "step: 528, loss: 0.014218256808817387\n",
      "step: 529, loss: 0.03718594089150429\n",
      "step: 530, loss: 0.018787004053592682\n",
      "step: 531, loss: 0.00940485019236803\n",
      "step: 532, loss: 0.01792748272418976\n",
      "step: 533, loss: 0.020202718675136566\n",
      "step: 534, loss: 0.024156024679541588\n",
      "step: 535, loss: 0.029514562338590622\n",
      "step: 536, loss: 0.0031958299223333597\n",
      "step: 537, loss: 0.027140183374285698\n",
      "step: 538, loss: 0.06740287691354752\n",
      "step: 539, loss: 0.016991831362247467\n",
      "step: 540, loss: 0.05374157056212425\n",
      "step: 541, loss: 0.01668212190270424\n",
      "step: 542, loss: 0.0030760481022298336\n",
      "step: 543, loss: 0.08049662411212921\n",
      "step: 544, loss: 0.006234988570213318\n",
      "step: 545, loss: 0.009144281968474388\n",
      "step: 546, loss: 0.041082318872213364\n",
      "step: 547, loss: 0.04552632197737694\n",
      "step: 548, loss: 0.006128298118710518\n",
      "step: 549, loss: 0.05770969018340111\n",
      "step: 550, loss: 0.11355135589838028\n",
      "step: 551, loss: 0.014955746941268444\n",
      "step: 552, loss: 0.10236202925443649\n",
      "step: 553, loss: 0.005576522555202246\n",
      "step: 554, loss: 0.012223257683217525\n",
      "step: 555, loss: 0.06922444701194763\n",
      "step: 556, loss: 0.02412412129342556\n",
      "step: 557, loss: 0.0365925133228302\n",
      "step: 558, loss: 0.003880432341247797\n",
      "step: 559, loss: 0.005548394285142422\n",
      "step: 560, loss: 0.003963187802582979\n",
      "step: 561, loss: 0.03639696538448334\n",
      "step: 562, loss: 0.002731575397774577\n",
      "step: 563, loss: 0.004569379612803459\n",
      "step: 564, loss: 0.020861761644482613\n",
      "step: 565, loss: 0.005067613907158375\n",
      "step: 566, loss: 0.17103071510791779\n",
      "step: 567, loss: 0.020050454884767532\n",
      "step: 568, loss: 0.0369885191321373\n",
      "step: 569, loss: 0.010658588260412216\n",
      "step: 570, loss: 0.04209601506590843\n",
      "step: 571, loss: 0.00772006344050169\n",
      "step: 572, loss: 0.05813517048954964\n",
      "step: 573, loss: 0.013290274888277054\n",
      "step: 574, loss: 0.007469251286238432\n",
      "step: 575, loss: 0.02020905539393425\n",
      "step: 576, loss: 0.1371501088142395\n",
      "step: 577, loss: 0.07430312037467957\n",
      "step: 578, loss: 0.007542960811406374\n",
      "step: 579, loss: 0.055367957800626755\n",
      "step: 580, loss: 0.005063965450972319\n",
      "step: 581, loss: 0.05652167648077011\n",
      "step: 582, loss: 0.006001138594001532\n",
      "step: 583, loss: 0.024519776925444603\n",
      "step: 584, loss: 0.031016452237963676\n",
      "step: 585, loss: 0.05158209800720215\n",
      "step: 586, loss: 0.042575787752866745\n",
      "step: 587, loss: 0.005140781868249178\n",
      "step: 588, loss: 0.058735452592372894\n",
      "step: 589, loss: 0.006967348977923393\n",
      "step: 590, loss: 0.0029492450412362814\n",
      "step: 591, loss: 0.3107980489730835\n",
      "step: 592, loss: 0.011862553656101227\n",
      "step: 593, loss: 0.010548925027251244\n",
      "step: 594, loss: 0.01023433730006218\n",
      "step: 595, loss: 0.0026940808165818453\n",
      "step: 596, loss: 0.003161664819344878\n",
      "step: 597, loss: 0.012206346727907658\n",
      "step: 598, loss: 0.005984524264931679\n",
      "step: 599, loss: 0.0076309447176754475\n",
      "step: 600, loss: 0.003995330538600683\n",
      "step: 601, loss: 0.0023407747503370047\n",
      "step: 602, loss: 0.005386818666011095\n",
      "step: 603, loss: 0.004614768084138632\n",
      "step: 604, loss: 0.02577175386250019\n",
      "step: 605, loss: 0.005902670789510012\n",
      "step: 606, loss: 0.00223831320181489\n",
      "step: 607, loss: 0.004819841124117374\n",
      "step: 608, loss: 0.00423009879887104\n",
      "step: 609, loss: 0.0034602435771375895\n",
      "step: 610, loss: 0.003258208744227886\n",
      "step: 611, loss: 0.003001553937792778\n",
      "step: 612, loss: 0.005898789037019014\n",
      "step: 613, loss: 0.0022892123088240623\n",
      "step: 614, loss: 0.006584668066352606\n",
      "step: 615, loss: 0.040343500673770905\n",
      "step: 616, loss: 0.0023227431811392307\n",
      "step: 617, loss: 0.004038437269628048\n",
      "step: 618, loss: 0.025051739066839218\n",
      "step: 619, loss: 0.003359091468155384\n",
      "step: 620, loss: 0.00361587293446064\n",
      "step: 621, loss: 0.003850623033940792\n",
      "step: 622, loss: 0.006069681607186794\n",
      "step: 623, loss: 0.0021727595012634993\n",
      "step: 624, loss: 0.007792804390192032\n",
      "step: 625, loss: 0.002052048686891794\n",
      "step: 626, loss: 0.009789169766008854\n",
      "step: 627, loss: 0.0027197522576898336\n",
      "step: 628, loss: 0.0036469162441790104\n",
      "step: 629, loss: 0.02715345472097397\n",
      "step: 630, loss: 0.0015837934333831072\n",
      "step: 631, loss: 0.0010233708890154958\n",
      "step: 632, loss: 0.002456000540405512\n",
      "step: 633, loss: 0.002225690521299839\n",
      "step: 634, loss: 0.016328739002346992\n",
      "step: 635, loss: 0.004615048412233591\n",
      "step: 636, loss: 0.00420214980840683\n",
      "step: 637, loss: 0.0033598034642636776\n",
      "step: 638, loss: 0.00497464369982481\n",
      "step: 639, loss: 0.00259014661423862\n",
      "step: 640, loss: 0.003969024866819382\n",
      "step: 641, loss: 0.04701811447739601\n",
      "step: 642, loss: 0.0027951339725404978\n",
      "step: 643, loss: 0.0029799959156662226\n",
      "step: 644, loss: 0.0035119331441819668\n",
      "step: 645, loss: 0.003194510005414486\n",
      "step: 646, loss: 0.007393852341920137\n",
      "step: 647, loss: 0.00786992534995079\n",
      "step: 648, loss: 0.00569124286994338\n",
      "step: 649, loss: 0.05520093813538551\n",
      "step: 650, loss: 0.0022718904074281454\n",
      "step: 651, loss: 0.005085748620331287\n",
      "step: 652, loss: 0.004106905311346054\n",
      "step: 653, loss: 0.004984070546925068\n",
      "step: 654, loss: 0.0034555429592728615\n",
      "step: 655, loss: 0.0034206202253699303\n",
      "step: 656, loss: 0.005559885408729315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 657, loss: 0.0027697542682290077\n",
      "step: 658, loss: 0.010950446128845215\n",
      "step: 659, loss: 0.00300815817900002\n",
      "step: 660, loss: 0.07831110805273056\n",
      "step: 661, loss: 0.007019403390586376\n",
      "step: 662, loss: 0.00450220238417387\n",
      "step: 663, loss: 0.00362885813228786\n",
      "step: 664, loss: 0.006481174845248461\n",
      "step: 665, loss: 0.0019053752766922116\n",
      "step: 666, loss: 0.0035842100623995066\n",
      "step: 667, loss: 0.0017532057827338576\n",
      "step: 668, loss: 0.0020501387771219015\n",
      "step: 669, loss: 0.01833849959075451\n",
      "step: 670, loss: 0.003580831689760089\n",
      "step: 671, loss: 0.0022789337672293186\n",
      "step: 672, loss: 0.0037378696724772453\n",
      "step: 673, loss: 0.0029292625840753317\n",
      "step: 674, loss: 0.00433876970782876\n",
      "step: 675, loss: 0.004101177211850882\n",
      "step: 676, loss: 0.013980414718389511\n",
      "step: 677, loss: 0.0020049954764544964\n",
      "step: 678, loss: 0.0029655920807272196\n",
      "step: 679, loss: 0.0013799674343317747\n",
      "step: 680, loss: 0.0011958974646404386\n",
      "step: 681, loss: 0.0017528739990666509\n",
      "step: 682, loss: 0.008519329130649567\n",
      "step: 683, loss: 0.0029340090695768595\n",
      "step: 684, loss: 0.006087739020586014\n",
      "step: 685, loss: 0.0036916029639542103\n",
      "step: 686, loss: 0.0025901079643517733\n",
      "step: 687, loss: 0.0014617423294112086\n",
      "step: 688, loss: 0.0038481904193758965\n",
      "step: 689, loss: 0.020853687077760696\n",
      "step: 690, loss: 0.002709819469600916\n",
      "step: 691, loss: 0.0017749490216374397\n",
      "step: 692, loss: 0.003969913814216852\n",
      "step: 693, loss: 0.002362470841035247\n",
      "step: 694, loss: 0.0024630443658679724\n",
      "step: 695, loss: 0.005339460447430611\n",
      "step: 696, loss: 0.009985917247831821\n",
      "step: 697, loss: 0.002374476520344615\n",
      "step: 698, loss: 0.07072760909795761\n",
      "step: 699, loss: 0.0026236772537231445\n",
      "step: 700, loss: 0.009501227177679539\n",
      "step: 701, loss: 0.001496054115705192\n",
      "step: 702, loss: 0.0325809009373188\n",
      "step: 703, loss: 0.0042083426378667355\n",
      "step: 704, loss: 0.0032398987095803022\n",
      "step: 705, loss: 0.0815902128815651\n",
      "step: 706, loss: 0.004854903090745211\n",
      "step: 707, loss: 0.0034527918323874474\n",
      "step: 708, loss: 0.003202638356015086\n",
      "step: 709, loss: 0.04563014581799507\n",
      "step: 710, loss: 0.0033248504623770714\n",
      "step: 711, loss: 0.01600789837539196\n",
      "step: 712, loss: 0.0030395060312002897\n",
      "step: 713, loss: 0.002207312034443021\n",
      "step: 714, loss: 0.0030573843978345394\n",
      "step: 715, loss: 0.003231014823541045\n",
      "step: 716, loss: 0.002865411574020982\n",
      "step: 717, loss: 0.0019705642480403185\n",
      "step: 718, loss: 0.00785086490213871\n",
      "step: 719, loss: 0.0021316464990377426\n",
      "step: 720, loss: 0.004296020604670048\n",
      "step: 721, loss: 0.003030141582712531\n",
      "step: 722, loss: 0.006967390887439251\n",
      "step: 723, loss: 0.0017046475550159812\n",
      "step: 724, loss: 0.002820743713527918\n",
      "step: 725, loss: 0.002312786178663373\n",
      "step: 726, loss: 0.0026674973778426647\n",
      "step: 727, loss: 0.00670969020575285\n",
      "step: 728, loss: 0.0026023266837000847\n",
      "step: 729, loss: 0.002963210688903928\n",
      "step: 730, loss: 0.0019337399862706661\n",
      "step: 731, loss: 0.04242442548274994\n",
      "step: 732, loss: 0.020033173263072968\n",
      "step: 733, loss: 0.0017348839901387691\n",
      "step: 734, loss: 0.004002197179943323\n",
      "step: 735, loss: 0.0011308883549645543\n",
      "step: 736, loss: 0.0022346789482980967\n",
      "step: 737, loss: 0.001273183966986835\n",
      "step: 738, loss: 0.08652517944574356\n",
      "step: 739, loss: 0.009602492675185204\n",
      "step: 740, loss: 0.0022127211559563875\n",
      "step: 741, loss: 0.00963887944817543\n",
      "step: 742, loss: 0.001853184076026082\n",
      "step: 743, loss: 0.019072139635682106\n",
      "step: 744, loss: 0.010247297585010529\n",
      "step: 745, loss: 0.0015279644867405295\n",
      "step: 746, loss: 0.004136296920478344\n",
      "step: 747, loss: 0.042205799371004105\n",
      "step: 748, loss: 0.017333317548036575\n",
      "step: 749, loss: 0.003003300167620182\n",
      "step: 750, loss: 0.00229255692102015\n",
      "step: 751, loss: 0.0033774098847061396\n",
      "step: 752, loss: 0.0028513383585959673\n",
      "step: 753, loss: 0.002091730944812298\n",
      "step: 754, loss: 0.0018181157065555453\n",
      "step: 755, loss: 0.006348676048219204\n",
      "step: 756, loss: 0.003722720080986619\n",
      "step: 757, loss: 0.004077509511262178\n",
      "step: 758, loss: 0.0035440183710306883\n",
      "step: 759, loss: 0.0007929586572572589\n",
      "step: 760, loss: 0.003087641205638647\n",
      "step: 761, loss: 0.0027819082606583834\n",
      "step: 762, loss: 0.03195898234844208\n",
      "step: 763, loss: 0.004554560873657465\n",
      "step: 764, loss: 0.004717271309345961\n",
      "step: 765, loss: 0.00352556467987597\n",
      "step: 766, loss: 0.001495343865826726\n",
      "step: 767, loss: 0.0026347048114985228\n",
      "step: 768, loss: 0.0015911689260974526\n",
      "step: 769, loss: 0.0012077182764187455\n",
      "step: 770, loss: 0.0014804298989474773\n",
      "step: 771, loss: 0.0031862014438956976\n",
      "step: 772, loss: 0.0009325537248514593\n",
      "step: 773, loss: 0.0014096739469096065\n",
      "step: 774, loss: 0.0037653944455087185\n",
      "step: 775, loss: 0.0020770058035850525\n",
      "step: 776, loss: 0.0012692679883912206\n",
      "step: 777, loss: 0.0015926319174468517\n",
      "step: 778, loss: 0.002610461786389351\n",
      "step: 779, loss: 0.0013341122539713979\n",
      "step: 780, loss: 0.0016190081369131804\n",
      "step: 781, loss: 0.001376502332277596\n",
      "step: 782, loss: 0.003377701388671994\n",
      "step: 783, loss: 0.0018831960624083877\n",
      "step: 784, loss: 0.0013192398473620415\n",
      "step: 785, loss: 0.0006807799800299108\n",
      "step: 786, loss: 0.0011802100343629718\n",
      "step: 787, loss: 0.0013331120135262609\n",
      "step: 788, loss: 0.00288527668453753\n",
      "step: 789, loss: 0.0005332291475497186\n",
      "step: 790, loss: 0.0035718739964067936\n",
      "step: 791, loss: 0.0008774097077548504\n",
      "step: 792, loss: 0.001659352332353592\n",
      "step: 793, loss: 0.0016373605467379093\n",
      "step: 794, loss: 0.0010692597134038806\n",
      "step: 795, loss: 0.00615647854283452\n",
      "step: 796, loss: 0.0007466142415069044\n",
      "step: 797, loss: 0.0026359453331679106\n",
      "step: 798, loss: 0.002445810940116644\n",
      "step: 799, loss: 0.0014109296025708318\n",
      "step: 800, loss: 0.001567732891999185\n",
      "step: 801, loss: 0.06072162091732025\n",
      "step: 802, loss: 0.0011632955865934491\n",
      "step: 803, loss: 0.008904596790671349\n",
      "step: 804, loss: 0.0023737840820103884\n",
      "step: 805, loss: 0.0014101358829066157\n",
      "step: 806, loss: 0.0018948604119941592\n",
      "step: 807, loss: 0.0010761901503428817\n",
      "step: 808, loss: 0.0007807331276126206\n",
      "step: 809, loss: 0.0009498216095380485\n",
      "step: 810, loss: 0.005747923627495766\n",
      "step: 811, loss: 0.0010928901610895991\n",
      "step: 812, loss: 0.0012339538661763072\n",
      "step: 813, loss: 0.001125983544625342\n",
      "step: 814, loss: 0.0020157841499894857\n",
      "step: 815, loss: 0.0009644932579249144\n",
      "step: 816, loss: 0.0012584966607391834\n",
      "step: 817, loss: 0.0031921558547765017\n",
      "step: 818, loss: 0.003113366663455963\n",
      "step: 819, loss: 0.001731747412122786\n",
      "step: 820, loss: 0.0015764333074912429\n",
      "step: 821, loss: 0.26981738209724426\n",
      "step: 822, loss: 0.0016518435440957546\n",
      "step: 823, loss: 0.0019412183901295066\n",
      "step: 824, loss: 0.0013548689894378185\n",
      "step: 825, loss: 0.0011277192970737815\n",
      "step: 826, loss: 0.0014203505124896765\n",
      "step: 827, loss: 0.0021191597916185856\n",
      "step: 828, loss: 0.0013531644362956285\n",
      "step: 829, loss: 0.0049882009625434875\n",
      "step: 830, loss: 0.0012807759921997786\n",
      "step: 831, loss: 0.0009112685802392662\n",
      "step: 832, loss: 0.0008746250532567501\n",
      "step: 833, loss: 0.0012700798688456416\n",
      "step: 834, loss: 0.0027534645050764084\n",
      "step: 835, loss: 0.0009583651553839445\n",
      "step: 836, loss: 0.0015320309903472662\n",
      "step: 837, loss: 0.001626399578526616\n",
      "step: 838, loss: 0.0013803720939904451\n",
      "step: 839, loss: 0.0019196305656805634\n",
      "step: 840, loss: 0.0007852694252505898\n",
      "step: 841, loss: 0.001299380324780941\n",
      "step: 842, loss: 0.0015912242233753204\n",
      "step: 843, loss: 0.000967266212683171\n",
      "step: 844, loss: 0.0010424916399642825\n",
      "step: 845, loss: 0.10199134796857834\n",
      "step: 846, loss: 0.03453834727406502\n",
      "step: 847, loss: 0.0016926336102187634\n",
      "step: 848, loss: 0.015973208472132683\n",
      "step: 849, loss: 0.0009323061094619334\n",
      "step: 850, loss: 0.0019475211156532168\n",
      "step: 851, loss: 0.060785409063100815\n",
      "step: 852, loss: 0.0032112491317093372\n",
      "step: 853, loss: 0.0011197739513590932\n",
      "step: 854, loss: 0.0006180406198836863\n",
      "step: 855, loss: 0.000831915414892137\n",
      "step: 856, loss: 0.0012402439024299383\n",
      "step: 857, loss: 0.0007194822537712753\n",
      "step: 858, loss: 0.001747520174831152\n",
      "step: 859, loss: 0.0015201644273474813\n",
      "step: 860, loss: 0.0014995862729847431\n",
      "step: 861, loss: 0.001461828825995326\n",
      "step: 862, loss: 0.00269888062030077\n",
      "step: 863, loss: 0.05011950805783272\n",
      "step: 864, loss: 0.0012927140342071652\n",
      "step: 865, loss: 0.0012900552246719599\n",
      "step: 866, loss: 0.0011281827464699745\n",
      "step: 867, loss: 0.0014019906520843506\n",
      "step: 868, loss: 0.0007389027159661055\n",
      "step: 869, loss: 0.004238600842654705\n",
      "step: 870, loss: 0.0010460689663887024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 871, loss: 0.0018143121851608157\n",
      "step: 872, loss: 0.002906119916588068\n",
      "step: 873, loss: 0.001244469080120325\n",
      "step: 874, loss: 0.003525001695379615\n",
      "step: 875, loss: 0.0015288625145331025\n",
      "step: 876, loss: 0.0012394734658300877\n",
      "step: 877, loss: 0.0020825183019042015\n",
      "step: 878, loss: 0.0007576397038064897\n",
      "step: 879, loss: 0.000971272645983845\n",
      "step: 880, loss: 0.0026292740367352962\n",
      "step: 881, loss: 0.001222278573550284\n",
      "step: 882, loss: 0.002984805963933468\n",
      "step: 883, loss: 0.03071279264986515\n",
      "step: 884, loss: 0.006006630603224039\n",
      "step: 885, loss: 0.0012730308808386326\n",
      "step: 886, loss: 0.0009857297409325838\n",
      "step: 887, loss: 0.0011202696477994323\n",
      "step: 888, loss: 0.0007854471914470196\n",
      "step: 889, loss: 0.0010985153494402766\n",
      "step: 890, loss: 0.002887729788199067\n",
      "step: 891, loss: 0.0024731471203267574\n",
      "step: 892, loss: 0.0027556156273931265\n",
      "step: 893, loss: 0.0009012080845423043\n",
      "step: 894, loss: 0.0016295632813125849\n",
      "step: 895, loss: 0.005287406034767628\n",
      "step: 896, loss: 0.0038424422964453697\n",
      "step: 897, loss: 0.0017333178548142314\n",
      "step: 898, loss: 0.0010699108242988586\n",
      "step: 899, loss: 0.001837721560150385\n",
      "step: 900, loss: 0.0009852147195488214\n",
      "step: 901, loss: 0.0015286956913769245\n",
      "step: 902, loss: 0.0011438546935096383\n",
      "step: 903, loss: 0.05274534225463867\n",
      "step: 904, loss: 0.0007727511110715568\n",
      "step: 905, loss: 0.00930729415267706\n",
      "step: 906, loss: 0.0015852877404540777\n",
      "step: 907, loss: 0.0007964359829202294\n",
      "step: 908, loss: 0.0010906611569225788\n",
      "step: 909, loss: 0.0012840195558965206\n",
      "step: 910, loss: 0.0012560541508719325\n",
      "step: 911, loss: 0.0015270155854523182\n",
      "step: 912, loss: 0.0010951734147965908\n",
      "step: 913, loss: 0.0005974044324830174\n",
      "step: 914, loss: 0.0014039523666724563\n",
      "step: 915, loss: 0.0019678666722029448\n",
      "step: 916, loss: 0.0013184886192902923\n",
      "step: 917, loss: 0.001970105804502964\n",
      "step: 918, loss: 0.000935335410758853\n",
      "step: 919, loss: 0.000972159206867218\n",
      "step: 920, loss: 0.0010362607426941395\n",
      "step: 921, loss: 0.0011215355480089784\n",
      "step: 922, loss: 0.002582588465884328\n",
      "step: 923, loss: 0.0013014489086344838\n",
      "step: 924, loss: 0.0007954754400998354\n",
      "step: 925, loss: 0.0014738410245627165\n",
      "step: 926, loss: 0.0010293340310454369\n",
      "step: 927, loss: 0.0017294548451900482\n",
      "step: 928, loss: 0.001069902442395687\n",
      "step: 929, loss: 0.0011094962246716022\n",
      "step: 930, loss: 0.0006944796186871827\n",
      "step: 931, loss: 0.0012425470631569624\n",
      "step: 932, loss: 0.001018159557133913\n",
      "step: 933, loss: 0.0010487837716937065\n",
      "step: 934, loss: 0.0008287320379167795\n",
      "step: 935, loss: 0.0019788213539868593\n",
      "step: 936, loss: 0.001189501490443945\n",
      "step: 937, loss: 0.0017368585104122758\n",
      "step: 938, loss: 0.0006763754063285887\n",
      "step: 939, loss: 0.0012438540579751134\n",
      "step: 940, loss: 0.0013000473845750093\n",
      "step: 941, loss: 0.0008024921407923102\n",
      "step: 942, loss: 0.0007547854911535978\n",
      "step: 943, loss: 0.0010906326351687312\n",
      "step: 944, loss: 0.002909182570874691\n",
      "step: 945, loss: 0.0008400095975957811\n",
      "step: 946, loss: 0.002695640316233039\n",
      "step: 947, loss: 0.0016476912423968315\n",
      "step: 948, loss: 0.00184973469004035\n",
      "step: 949, loss: 0.0036634544376283884\n",
      "step: 950, loss: 0.0018562899203971028\n",
      "step: 951, loss: 0.00052498874720186\n",
      "step: 952, loss: 0.0018405599985271692\n",
      "step: 953, loss: 0.000593998993281275\n",
      "step: 954, loss: 0.0013855433790013194\n",
      "step: 955, loss: 0.0017968681640923023\n",
      "step: 956, loss: 0.001536174095235765\n",
      "step: 957, loss: 0.0005934381042607129\n",
      "step: 958, loss: 0.0015611700946465135\n",
      "step: 959, loss: 0.0008185126353055239\n",
      "step: 960, loss: 0.0349600687623024\n",
      "step: 961, loss: 0.0013291655341163278\n",
      "step: 962, loss: 0.0007161376415751874\n",
      "step: 963, loss: 0.000726854195818305\n",
      "step: 964, loss: 0.0028920627664774656\n",
      "step: 965, loss: 0.0006883967434987426\n",
      "step: 966, loss: 0.0007352926768362522\n",
      "step: 967, loss: 0.0008047651499509811\n",
      "step: 968, loss: 0.0008310883422382176\n",
      "step: 969, loss: 0.00042507401667535305\n",
      "step: 970, loss: 0.001305044861510396\n",
      "step: 971, loss: 0.0016232216730713844\n",
      "step: 972, loss: 0.000914969015866518\n",
      "step: 973, loss: 0.002233378356322646\n",
      "step: 974, loss: 0.0019122757948935032\n",
      "step: 975, loss: 0.0012762643164023757\n",
      "step: 976, loss: 0.0013454112922772765\n",
      "step: 977, loss: 0.002606563037261367\n",
      "step: 978, loss: 0.0007354066474363208\n",
      "step: 979, loss: 0.0005107102915644646\n",
      "step: 980, loss: 0.0005776080652140081\n",
      "step: 981, loss: 0.0007604347192682326\n",
      "step: 982, loss: 0.0009177132160402834\n",
      "step: 983, loss: 0.0017566489987075329\n",
      "step: 984, loss: 0.0009616903262212873\n",
      "step: 985, loss: 0.0014039106899872422\n",
      "step: 986, loss: 0.001019385876134038\n",
      "step: 987, loss: 0.0010670262854546309\n",
      "step: 988, loss: 0.0007991191814653575\n",
      "step: 989, loss: 0.0005487797898240387\n",
      "step: 990, loss: 0.00043965294025838375\n",
      "step: 991, loss: 0.0015995402354747057\n",
      "step: 992, loss: 0.00036419148091226816\n",
      "step: 993, loss: 0.028495360165834427\n",
      "step: 994, loss: 0.0008599854190833867\n",
      "step: 995, loss: 0.06287731975317001\n",
      "step: 996, loss: 0.000584520457778126\n",
      "step: 997, loss: 0.0006247429992072284\n",
      "step: 998, loss: 0.0067189945839345455\n",
      "step: 999, loss: 0.11725149303674698\n",
      "step: 1000, loss: 0.000812040118034929\n",
      "Epoch:6\n",
      "Accuracy on test data:0.7599434329149726\n"
     ]
    }
   ],
   "source": [
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape = [None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape = [None, ])\n",
    "        \n",
    "        weights_1 = tf.get_variable(\n",
    "            name = 'weights_input_hidden',\n",
    "            shape = (self._vocab_size, self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed = 2018),\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name = 'biases_input_hidden',\n",
    "            shape = (self._hidden_size),\n",
    "            initializer = tf.random_normal_initializer(seed = 2018),\n",
    "        )\n",
    "        weights_2 = tf.get_variable(\n",
    "            name = 'weights_hidden_output',\n",
    "            shape = (self._hidden_size, NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed = 2018),\n",
    "        )\n",
    "        biases_2 = tf.get_variable(\n",
    "            name = \"biases_hidden_output\",\n",
    "            shape = (NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed = 2018),\n",
    "        )\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        labels_one_hot = tf.one_hot(indices = self._real_Y, depth = NUM_CLASSES, dtype = tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels = labels_one_hot, logits = logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis = 1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "\n",
    "        return predicted_labels, loss\n",
    " \n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "        return train_op\n",
    "   \n",
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._size = len(self._data)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2018)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]\n",
    "#Load data\n",
    "def load_data():\n",
    "    train_data_reader = DataReader(\n",
    "    data_path = 'D:/DSLab/DSLab_Training_Phase1/Dataset/20news-bydate/20news-train-tfidf.txt',\n",
    "    batch_size = 50,\n",
    "    vocab_size=vocab_size\n",
    "  )\n",
    "    test_data_reader = DataReader(\n",
    "    data_path = 'D:/DSLab/DSLab_Training_Phase1/Dataset/20news-bydate/20news-test-tfidf.txt',\n",
    "    batch_size = 50,\n",
    "    vocab_size = vocab_size\n",
    "  )\n",
    "    return train_data_reader, test_data_reader\n",
    "\n",
    "def save_parameters(name, value, epoch):\n",
    "        filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "        if len(value.shape) == 1: #is a list\n",
    "            string_form = ','.join([str(number) for number in value])\n",
    "        else:\n",
    "            string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "        with open('D:/DSLab/DSLab_Training_Phase1/Dataset/20news-bydate/saved-paras/' + filename, 'w') as f:\n",
    "            f.write(string_form)\n",
    "def restore_parameters(name, epoch):\n",
    "        filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "        with open(\"D:/DSLab/DSLab_Training_Phase1/Dataset/20news-bydate/saved-paras/\" + filename) as f:\n",
    "            lines = f.read().splitlines()\n",
    "        if len(lines) == 1:\n",
    "            value = [float(number) for number in lines[0].split(',')]\n",
    "        else:\n",
    "            value = [[float(number) for number in lines[rows].split(',')] for rows in range(len(lines))]\n",
    "        return value\n",
    "\n",
    "with open('D:/DSLab/DSLab_Training_Phase1/Dataset/20news-bydate/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "  \n",
    "NUM_CLASSES = 20\n",
    "\n",
    "mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
    "\n",
    "train_data_reader, test_data_reader = load_data()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    step, MAX_STEP = 0, 1000\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        plabels_eval, loss_eval, _ = sess.run([predicted_labels, loss, train_op], feed_dict = {mlp._X: train_data, mlp._real_Y: train_labels})\n",
    "        step += 1\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        for variable in trainable_variables:\n",
    "            save_parameters(name = variable.name, value = variable.eval(), epoch = train_data_reader._num_epoch)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    epoch = train_data_reader._num_epoch\n",
    "\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "\n",
    "    num_true_preds = 0\n",
    "    while True:\n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_plabels_eval = sess.run(predicted_labels, feed_dict = { mlp._X: test_data, mlp._real_Y: test_labels})\n",
    "        matches = np.equal(test_plabels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "      \n",
    "print('Epoch:{}'.format(epoch))\n",
    "print('Accuracy on test data:{}'.format(num_true_preds / len(test_data_reader._data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
